Lets first try a small dataset of English as a sanity check My favorite fun dataset is the concatenation of Paul Graham’s essays The basic idea is that there’s a lot of wisdom in these essaysg but unfortunately Paul Graham is a relatively slow generator Wouldn’t it be great if we could sample startup wisdom on demand That’s where an RNN comes in Concatenating all pg essays over the last ~5 years we get approximately 1MB text file or about 1 million characters this is considered a very small dataset by the way Technical: Lets train a 2-layer LSTM with 512 hidden nodes approx 35 million parameters and with dropout of 05 after each layer We’ll train with batches of 100 examples and truncated backpropagation through time of length 100 characters With these settings one batch on a TITAN Z GPU takes about 046 seconds this can be cut in half with 50 character BPTT at negligible cost in performance Without further ado lets see a sample from the RNN
